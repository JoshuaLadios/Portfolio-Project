{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d72d470",
   "metadata": {},
   "source": [
    "### The Goal of this Project is to create a Fraud Detection Pipeline That follows certain Rule (Client Requirements)\n",
    "#### 1. Transactions that are dated before 2000 are highly likely to be a Fraud transaction.\n",
    "#### 2. Bulk Transactions that are more than 500 (Quantity) are highly likely to be a Fraud Transaction.\n",
    "#### 3. Transactions that costs 0 but tagged as Complete in Status Column are highly likely to be a Fraud Transaction.\n",
    "#### 4. Transactions that are tagged as \"Unknown\" in Payment method are highly likely to be a Fraud transaction.\n",
    "#### 5. Transactions with \"Unknown\" status in (transaction_status) column are highly likely to be a Fraud transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7b1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "import csv\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499978bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'DB_Financial_Transaction' ensured to exist.\n",
      "Connected to SQL Server Successfully!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "server = os.getenv('DB_SERVER')\n",
    "database = os.getenv('DB_NAME')\n",
    "driver = os.getenv('DB_DRIVER').replace(' ', '+')\n",
    "\n",
    "master_conn_str = f\"mssql+pyodbc://{server}/master?trusted_connection=yes&driver={driver}\"\n",
    "engine_master = create_engine(master_conn_str, isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "with engine_master.connect() as conn:\n",
    "    conn.execute(text(f\"IF DB_ID(N'{database}') IS NULL CREATE DATABASE [{database}]\"))\n",
    "    print(f\"Database '{database}' ensured to exist.\")\n",
    "\n",
    "target_conn_str = f\"mssql+pyodbc://{server}/{database}?trusted_connection=yes&driver={driver}\"\n",
    "engine = create_engine(target_conn_str)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connected to SQL Server Successfully!\")\n",
    "        conn.close()\n",
    "except Exception as e:\n",
    "    print(\"Error Connecting to SQL Server:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189f73a",
   "metadata": {},
   "source": [
    "### Logging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7585c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"pipeline.log\"\n",
    "logging.basicConfig(\n",
    "    filename=LOG_PATH,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9acec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_delimiter(file, sample_size=1024):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        sample = f.read(sample_size)\n",
    "        sniffer = csv.Sniffer()\n",
    "        try:\n",
    "            dialect = sniffer.sniff(sample)\n",
    "            return dialect.delimiter\n",
    "        except csv.Error:\n",
    "            return \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c14829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file):\n",
    "    try:\n",
    "        ext = os.path.splitext(file)[1].lower()\n",
    "\n",
    "        if ext in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file)\n",
    "        elif ext == '.csv':\n",
    "            delimiter = detect_delimiter(file)\n",
    "            # print(f\"Detected delimiter for {file}: {repr(delimiter)}\")\n",
    "            df = pd.read_csv(file, delimiter=delimiter)\n",
    "        else:\n",
    "            print(f\"Unsupported file: {file}\")\n",
    "            return None\n",
    "    \n",
    "        if df is None or df.empty:\n",
    "            print(\"File is empty or constains no Data\")\n",
    "\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file}': {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8595698",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_formats = [\n",
    "    \"%Y-%m-%d\",    # 2025-10-13\n",
    "    \"%Y-%d-%m\",    # 2025-13-10\n",
    "    \"%m-%d-%Y\",    # 10-13-2025\n",
    "    \"%d-%m-%Y\",    # 13-10-2025\n",
    "    \"%b %d, %Y\",   # Oct 13, 2025\n",
    "    \"%B %d, %Y\"    # October 13, 2025\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec81505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return pd.to_datetime(\"1900-01-01\")\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str.strip(), fmt)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return pd.to_datetime(\"1900-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34dfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_columns = {\n",
    "    'transaction_id',\n",
    "    'transaction_date',\n",
    "    'customer_id',\n",
    "    'product_name',\n",
    "    'quantity',\n",
    "    'price',\n",
    "    'payment_method',\n",
    "    'transaction_status'\n",
    "}\n",
    "\n",
    "valid_product = ['Headphones', 'Coffee Machine', 'Smartphone', 'Laptop', 'Tablet']\n",
    "valid_payment_methods = ['Paypal', 'Creditcard', 'Cash']\n",
    "valid_transaction_status = ['Pending', 'Completed', 'Failed']\n",
    "column_dict = ['transaction_id', 'product_name', 'quantity', 'price', 'total_sales', 'customer_id', 'payment_method', 'transaction_date', 'transaction_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b6b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"C\": \"Coffee Machine\",\n",
    "    \"Co\": \"Coffee Machine\",\n",
    "    \"Cof\": \"Coffee Machine\",\n",
    "    \"Coff\": \"Coffee Machine\",\n",
    "    \"Coffe\": \"Coffee Machine\",\n",
    "    \"Coffee\": \"Coffee Machine\",\n",
    "    \"Coffee \": \"Coffee Machine\",\n",
    "    \"Coffee M\": \"Coffee Machine\",\n",
    "    \"Coffee Ma\": \"Coffee Machine\",\n",
    "    \"Coffee Mac\": \"Coffee Machine\",\n",
    "    \"Coffee Mach\": \"Coffee Machine\",\n",
    "    \"Coffee Machi\": \"Coffee Machine\",\n",
    "    \"Coffee Machin\": \"Coffee Machine\",\n",
    "    \"H\": \"Headphones\",\n",
    "    \"He\": \"Headphones\",\n",
    "    \"Hea\": \"Headphones\",\n",
    "    \"Head\": \"Headphones\",\n",
    "    \"Headp\": \"Headphones\",\n",
    "    \"Headph\": \"Headphones\",\n",
    "    \"Headpho\": \"Headphones\",\n",
    "    \"Headphon\": \"Headphones\",\n",
    "    \"Headphone\": \"Headphones\",\n",
    "    \"T\": \"Tablet\",\n",
    "    \"Ta\": \"Tablet\",\n",
    "    \"Tab\": \"Tablet\",\n",
    "    \"Tabl\": \"Tablet\",\n",
    "    \"Table\": \"Tablet\",\n",
    "    \"S\": \"Smartphone\",\n",
    "    \"Sm\": \"Smartphone\",\n",
    "    \"Sma\": \"Smartphone\",\n",
    "    \"Smar\": \"Smartphone\",\n",
    "    \"Smart\": \"Smartphone\",\n",
    "    \"Smartp\": \"Smartphone\",\n",
    "    \"Smartph\": \"Smartphone\",\n",
    "    \"Smartpho\": \"Smartphone\",\n",
    "    \"Smartphon\": \"Smartphone\",\n",
    "    \"L\": \"Laptop\",\n",
    "    \"La\": \"Laptop\",\n",
    "    \"Lap\": \"Laptop\",\n",
    "    \"Lapt\": \"Laptop\",\n",
    "    \"Lapto\": \"Laptop\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e5888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_product_name(product_name_col):\n",
    "    product_name_col = product_name_col.str.replace(r'[^A-Za-z ]', '', regex=True)\n",
    "    product_name_col = product_name_col.str.title()\n",
    "    product_name_col = product_name_col.replace(mapping)\n",
    "    # product_name_col = product_name_col.fillna('Unknown')\n",
    "    product_name_col = product_name_col.where(product_name_col.isin(valid_product), 'Unknown')\n",
    "    return product_name_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f670cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_quantity(item_quantity):\n",
    "    item_quantity = pd.to_numeric(item_quantity, errors='coerce')\n",
    "    item_quantity = item_quantity.fillna(0)\n",
    "    item_quantity = item_quantity.abs()\n",
    "    item_quantity = item_quantity.astype(int)\n",
    "    return item_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5f4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_price(price_col):\n",
    "    price_col = price_col.astype(str).str.replace(r'[^0-9.]', '', regex=True)\n",
    "    price_col = pd.to_numeric(price_col, errors='coerce')\n",
    "    price_col = price_col.fillna(0)\n",
    "    price_col = price_col.round(2).abs()\n",
    "    # price_col = '$' + price_col.astype(str)\n",
    "    return price_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319a83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_payment(payment_col):\n",
    "    payment_col = payment_col.str.lower().str.replace(' ', '').str.title()\n",
    "    # payment_col = payment_col.fillna('Unknown')\n",
    "    payment_col = payment_col.where(payment_col.isin(valid_payment_methods), 'Unknown')\n",
    "    return payment_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5b1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_transaction_status(transaction_status_col):\n",
    "    transaction_status_col = transaction_status_col.str.lower().str.replace(' ', '').str.strip().str.title()\n",
    "    transaction_status_col = transaction_status_col.replace(to_replace=r'.*Complete.*', value='Completed', regex=True)\n",
    "    # transaction_status_col = transaction_status_col.fillna('Unknown')\n",
    "    transaction_status_col = transaction_status_col.where(transaction_status_col.isin(valid_transaction_status), 'Unknown')\n",
    "    return transaction_status_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449a3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_detection(df):\n",
    "    #Rule-based Fraud Detection\n",
    "    df['rule_suspicious_product_name'] = ~df['product_name'].isin(valid_product)\n",
    "    df['rule_suspicious_date'] = df['transaction_date'] < \"2000-01-01\"\n",
    "    df['rule_bulk_quantity'] = df['quantity'] > 500\n",
    "    df['rule_zero_price_completed'] = (df['price'] == 0) & (df['transaction_status'] == \"Completed\")\n",
    "    df['rule_unknown_payment'] = df['payment_method'] == 'Unknown'\n",
    "    df['rule_unknown_status'] = df['transaction_status'] == 'Unknown'\n",
    "\n",
    "    df['fraud_score'] = (\n",
    "        df['rule_suspicious_product_name'] * 20 +\n",
    "        df['rule_suspicious_date'] * 25 +\n",
    "        df['rule_bulk_quantity'] * 5 +\n",
    "        df['rule_zero_price_completed'] * 20 +\n",
    "        df['rule_unknown_payment'] * 15 +\n",
    "        df['rule_unknown_status'] * 15\n",
    "    ) \n",
    "\n",
    "    df['possible_fraud'] = df['fraud_score'] >= 20\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "670ac5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    try:\n",
    "    # Initial Column Cleaning and Sorting\n",
    "        df = df.dropna(subset=['transaction_id', 'customer_id'])\n",
    "        df = df.drop_duplicates(subset=['transaction_id'])\n",
    "        df = df.sort_values('transaction_id', ascending=True)\n",
    "\n",
    "        # Fix transaction_date format\n",
    "        df['transaction_date'] = df['transaction_date'].str.strip()\n",
    "        df['transaction_date'] = df['transaction_date'].str.replace(r\"[/.-]\", \"-\", regex=True)\n",
    "        df['transaction_date'] = df['transaction_date'].apply(try_parse_date)\n",
    "        df['transaction_date'] = df['transaction_date'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # More Column Cleaning\n",
    "        df['product_name'] = cleaning_product_name(df['product_name'])\n",
    "        df['quantity'] = cleaning_quantity(df['quantity'])\n",
    "        df['price'] = cleaning_price(df['price'])\n",
    "        df['payment_method'] = cleaning_payment(df['payment_method'])\n",
    "        df['transaction_status'] = cleaning_transaction_status(df['transaction_status'])\n",
    "\n",
    "        df['total_sales'] = df['quantity'] * df['price']\n",
    "        \n",
    "        df = df[column_dict]\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Missing important Columns: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b18a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, folder, prefix):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    path = f\"{folder}/{prefix}_{timestamp}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file):\n",
    "    logging.info(f\"üöÄ Starting pipeline!\")\n",
    "\n",
    "    df_raw = read_files(file)\n",
    "    if df_raw.empty:\n",
    "        logging.error(\"File is empty or failed to load\")\n",
    "        return\n",
    "    \n",
    "    known, unknown = (\n",
    "        df_raw[list(valid_columns)],\n",
    "        df_raw[[c for c in df_raw.columns if c not in valid_columns]]\n",
    "    )\n",
    "\n",
    "    if not unknown.empty:\n",
    "        save_file(unknown, \"Review_Files\", \"Columns_To_Review\")\n",
    "\n",
    "    df_clean = data_cleaning(known)\n",
    "    df_final = fraud_detection(df_clean)\n",
    "\n",
    "    final_path = save_file(df_final, \"Final_Files\", \"Final_Cleaned_Fraud_Data\")\n",
    "\n",
    "    logging.info(f\"Pipeline complete ‚Üí Saved to {final_path}\")\n",
    "    print(f\"‚úî Pipeline completed successfully.\\nüìÅ Final File: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\"../DataSet/dirty_financial_transactions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
